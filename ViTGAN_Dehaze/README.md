# Multispectral Image Generation using Vision Transformer GAN

This project implements a Generative Adversarial Network (GAN) using Vision Transformers (ViT) as the discriminator and a convolutional generator. The goal is to generate new, synthetic multispectral satellite images that resemble real satellite images.

## Overview

Generative Adversarial Networks (GANs) consist of two main components: the generator and the discriminator. The generator creates images, while the discriminator evaluates them. The discriminator in this project is based on the Vision Transformer architecture, allowing it to efficiently process and evaluate multispectral satellite images in terms of their authenticity.

### Dataset

The dataset used comprises multispectral satellite images captured by satellites orbiting the Earth. These images contain multiple bands of data representing various wavelengths of electromagnetic radiation. The dataset includes images of different geographical features such as forests, water bodies, urban areas, and agricultural fields.

### Vision Transformer (ViT) as Discriminator

The Vision Transformer model used as a discriminator divides each multispectral satellite image into patches, encodes these patches, and processes them through multiple transformer layers. The output from the transformers is then used to determine if an image is real or generated by the generator.

### Generator Architecture

The generator is a convolutional neural network that uses upsampling techniques to generate multispectral satellite images from a latent space representation. It starts with a dense layer and progressively upsamples the feature map to the target size.

## Model Architecture

- **Generator**: Utilizes transposed convolutional layers to upscale the latent space input into a multispectral satellite image.
- **Discriminator (ViT)**: Utilizes patches extracted from input images, which are then processed by transformer layers to classify images as real or fake.

## Training Process

The training process involves alternating between training the discriminator and the generator. The discriminator learns to better distinguish between real and fake images, while the generator learns to produce more realistic images to fool the discriminator.

### Loss Functions

- **Adversarial Loss**: Ensures the generator produces realistic multispectral satellite images.
- **Discriminator Loss**: Ensures the discriminator accurately classifies real and fake images.

## Results

The effectiveness of the model is demonstrated by the discriminator's accuracy and the visual quality of generated multispectral satellite images. Example outputs will be saved during training at specified intervals.

## Conclusion

This section showcases the application of Vision Transformers in adversarial settings to generate new multispectral satellite images. The model's performance can be further enhanced by experimenting with different architectures, training longer, or using larger datasets.
